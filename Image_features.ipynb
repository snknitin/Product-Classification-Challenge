{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Image features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# matplotlib and seaborn for plotting\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "import seaborn as sns\n",
    "%matplotlib inline  \n",
    "\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def walk_up_folder(path, depth=1):\n",
    "    \"\"\"\n",
    "    Helper method to navigate the file system and get to the file location\n",
    "    \"\"\"\n",
    "    _cur_depth = 1        \n",
    "    while _cur_depth < depth:\n",
    "        path = os.path.dirname(path)\n",
    "        _cur_depth += 1\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_path = os.path.join(walk_up_folder(os.getcwd(), depth=1),'Data/product_data.json')\n",
    "\n",
    "with open(data_path, encoding='utf-8') as data_file:\n",
    "    data = json.loads(data_file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def blank_image():\n",
    "    \"\"\"\n",
    "    Loads a blank image indicating missing file\n",
    "    \"\"\"\n",
    "    return Image.fromarray(np.zeros((200,200,3), dtype=\"uint8\"), 'RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import PIL\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import urllib.request\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "images=[]\n",
    "for i in range(len(data)):\n",
    "    url=data[i]['images_url']\n",
    "    if url.startswith(\"//\"):\n",
    "        url =\"https://\"+url[2:]\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        img = Image.open(BytesIO(response.content))\n",
    "        # Making sure all images are of the same dimensions\n",
    "        if img.size!=(200,200):\n",
    "            img=img.resize((200, 200), PIL.Image.ANTIALIAS)\n",
    "        images.append(img)\n",
    "    except:\n",
    "        images.append(blank_image())\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMgAAADICAIAAAAiOjnJAAAAiklEQVR4nO3BAQEAAACCIP+vbkhA\nAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\nAADwYNWXAAG9rB+hAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=200x200 at 0x7F8FE6D1BBE0>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images[287]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 200, 3)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_to_array(images[287]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 247.,  247.,  247., ...,  249.,  249.,  249.],\n",
       "         [ 249.,  249.,  249., ...,   47.,   32.,   85.],\n",
       "         [  49.,   37.,   84., ...,  249.,  249.,  249.],\n",
       "         ..., \n",
       "         [   0.,    0.,   61., ...,  251.,  251.,  251.],\n",
       "         [ 250.,  250.,  250., ...,  251.,  251.,  251.],\n",
       "         [ 251.,  251.,  251., ...,  150.,  132.,   54.]],\n",
       "\n",
       "        [[   8.,    0.,  117., ...,  251.,  251.,  251.],\n",
       "         [ 250.,  250.,  250., ...,  251.,  251.,  251.],\n",
       "         [ 251.,  251.,  251., ...,  155.,  137.,   44.],\n",
       "         ..., \n",
       "         [ 183.,  234.,  197., ...,  226.,  217.,  233.],\n",
       "         [ 215.,  205.,  228., ...,  251.,  251.,  251.],\n",
       "         [ 250.,  250.,  250., ...,  178.,  229.,  192.]],\n",
       "\n",
       "        [[ 184.,  234.,  197., ...,  228.,  220.,  230.],\n",
       "         [ 212.,  202.,  227., ...,  251.,  251.,  251.],\n",
       "         [ 250.,  250.,  250., ...,  178.,  228.,  191.],\n",
       "         ..., \n",
       "         [ 248.,  248.,  248., ...,  250.,  255.,  255.],\n",
       "         [ 251.,  255.,  252., ...,  193.,  184.,  226.],\n",
       "         [ 192.,  183.,  227., ...,  248.,  248.,  248.]]]], dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_to_array(images[0]).reshape(-1).reshape(1,3,200,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = np.ndarray(shape=(1000, 3, 200, 200),dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 200)\n"
     ]
    }
   ],
   "source": [
    "print(images[300].size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 200, 4)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_to_array(images[330]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 200, 3)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "cv2.cvtColor(img_to_array(images[330]), cv2.COLOR_BGRA2BGR).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 200)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images[19].resize((200, 200), PIL.Image.ANTIALIAS).size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (200, 200, 3)\n",
      "1 (200, 200, 3)\n",
      "2 (200, 200, 3)\n",
      "3 (200, 200, 3)\n",
      "4 (200, 200, 3)\n",
      "5 (200, 200, 3)\n",
      "6 (200, 200, 3)\n",
      "7 (200, 200, 3)\n",
      "8 (200, 200, 3)\n",
      "9 (200, 200, 3)\n",
      "10 (200, 200, 3)\n",
      "11 (200, 200, 3)\n",
      "12 (200, 200, 3)\n",
      "13 (200, 200, 3)\n",
      "14 (200, 200, 3)\n",
      "15 (200, 200, 3)\n",
      "16 (200, 200, 3)\n",
      "17 (200, 200, 3)\n",
      "18 (200, 200, 3)\n",
      "19 (200, 200, 3)\n",
      "20 (200, 200, 3)\n",
      "21 (200, 200, 3)\n",
      "22 (200, 200, 3)\n",
      "23 (200, 200, 3)\n",
      "24 (200, 200, 3)\n",
      "25 (200, 200, 3)\n",
      "26 (200, 200, 3)\n",
      "27 (200, 200, 3)\n",
      "28 (200, 200, 3)\n",
      "29 (200, 200, 3)\n",
      "30 (200, 200, 3)\n",
      "31 (200, 200, 3)\n",
      "32 (200, 200, 3)\n",
      "33 (200, 200, 3)\n",
      "34 (200, 200, 3)\n",
      "35 (200, 200, 3)\n",
      "36 (200, 200, 3)\n",
      "37 (200, 200, 3)\n",
      "38 (200, 200, 3)\n",
      "39 (200, 200, 3)\n",
      "40 (200, 200, 3)\n",
      "41 (200, 200, 3)\n",
      "42 (200, 200, 3)\n",
      "43 (200, 200, 3)\n",
      "44 (200, 200, 3)\n",
      "45 (200, 200, 3)\n",
      "46 (200, 200, 3)\n",
      "47 (200, 200, 3)\n",
      "48 (200, 200, 3)\n",
      "49 (200, 200, 3)\n",
      "50 (200, 200, 3)\n",
      "51 (200, 200, 3)\n",
      "52 (200, 200, 3)\n",
      "53 (200, 200, 3)\n",
      "54 (200, 200, 3)\n",
      "55 (200, 200, 3)\n",
      "56 (200, 200, 3)\n",
      "57 (200, 200, 3)\n",
      "58 (200, 200, 3)\n",
      "59 (200, 200, 3)\n",
      "60 (200, 200, 3)\n",
      "61 (200, 200, 3)\n",
      "62 (200, 200, 3)\n",
      "63 (200, 200, 3)\n",
      "64 (200, 200, 3)\n",
      "65 (200, 200, 3)\n",
      "66 (200, 200, 3)\n",
      "67 (200, 200, 3)\n",
      "68 (200, 200, 3)\n",
      "69 (200, 200, 3)\n",
      "70 (200, 200, 3)\n",
      "71 (200, 200, 3)\n",
      "72 (200, 200, 3)\n",
      "73 (200, 200, 3)\n",
      "74 (200, 200, 3)\n",
      "75 (200, 200, 3)\n",
      "76 (200, 200, 3)\n",
      "77 (200, 200, 3)\n",
      "78 (200, 200, 3)\n",
      "79 (200, 200, 3)\n",
      "80 (200, 200, 3)\n",
      "81 (200, 200, 3)\n",
      "82 (200, 200, 3)\n",
      "83 (200, 200, 3)\n",
      "84 (200, 200, 3)\n",
      "85 (200, 200, 3)\n",
      "86 (200, 200, 3)\n",
      "87 (200, 200, 3)\n",
      "88 (200, 200, 3)\n",
      "89 (200, 200, 3)\n",
      "90 (200, 200, 3)\n",
      "91 (200, 200, 3)\n",
      "92 (200, 200, 3)\n",
      "93 (200, 200, 3)\n",
      "94 (200, 200, 3)\n",
      "95 (200, 200, 3)\n",
      "96 (200, 200, 3)\n",
      "97 (200, 200, 3)\n",
      "98 (200, 200, 3)\n",
      "99 (200, 200, 3)\n",
      "100 (200, 200, 3)\n",
      "101 (200, 200, 3)\n",
      "102 (200, 200, 3)\n",
      "103 (200, 200, 3)\n",
      "104 (200, 200, 3)\n",
      "105 (200, 200, 3)\n",
      "106 (200, 200, 3)\n",
      "107 (200, 200, 3)\n",
      "108 (200, 200, 3)\n",
      "109 (200, 200, 3)\n",
      "110 (200, 200, 3)\n",
      "111 (200, 200, 3)\n",
      "112 (200, 200, 3)\n",
      "113 (200, 200, 3)\n",
      "114 (200, 200, 3)\n",
      "115 (200, 200, 3)\n",
      "116 (200, 200, 3)\n",
      "117 (200, 200, 3)\n",
      "118 (200, 200, 3)\n",
      "119 (200, 200, 3)\n",
      "120 (200, 200, 3)\n",
      "121 (200, 200, 3)\n",
      "122 (200, 200, 3)\n",
      "123 (200, 200, 3)\n",
      "124 (200, 200, 3)\n",
      "125 (200, 200, 3)\n",
      "126 (200, 200, 3)\n",
      "127 (200, 200, 3)\n",
      "128 (200, 200, 3)\n",
      "129 (200, 200, 3)\n",
      "130 (200, 200, 3)\n",
      "131 (200, 200, 3)\n",
      "132 (200, 200, 3)\n",
      "133 (200, 200, 3)\n",
      "134 (200, 200, 3)\n",
      "135 (200, 200, 3)\n",
      "136 (200, 200, 3)\n",
      "137 (200, 200, 3)\n",
      "138 (200, 200, 3)\n",
      "139 (200, 200, 3)\n",
      "140 (200, 200, 3)\n",
      "141 (200, 200, 3)\n",
      "142 (200, 200, 3)\n",
      "143 (200, 200, 3)\n",
      "144 (200, 200, 3)\n",
      "145 (200, 200, 3)\n",
      "146 (200, 200, 3)\n",
      "147 (200, 200, 3)\n",
      "148 (200, 200, 3)\n",
      "149 (200, 200, 3)\n",
      "150 (200, 200, 3)\n",
      "151 (200, 200, 3)\n",
      "152 (200, 200, 3)\n",
      "153 (200, 200, 3)\n",
      "154 (200, 200, 3)\n",
      "155 (200, 200, 3)\n",
      "156 (200, 200, 3)\n",
      "157 (200, 200, 3)\n",
      "158 (200, 200, 3)\n",
      "159 (200, 200, 3)\n",
      "160 (200, 200, 3)\n",
      "161 (200, 200, 3)\n",
      "162 (200, 200, 3)\n",
      "163 (200, 200, 3)\n",
      "164 (200, 200, 3)\n",
      "165 (200, 200, 3)\n",
      "166 (200, 200, 3)\n",
      "167 (200, 200, 3)\n",
      "168 (200, 200, 3)\n",
      "169 (200, 200, 3)\n",
      "170 (200, 200, 3)\n",
      "171 (200, 200, 3)\n",
      "172 (200, 200, 3)\n",
      "173 (200, 200, 3)\n",
      "174 (200, 200, 3)\n",
      "175 (200, 200, 3)\n",
      "176 (200, 200, 3)\n",
      "177 (200, 200, 3)\n",
      "178 (200, 200, 3)\n",
      "179 (200, 200, 3)\n",
      "180 (200, 200, 3)\n",
      "181 (200, 200, 3)\n",
      "182 (200, 200, 3)\n",
      "183 (200, 200, 3)\n",
      "184 (200, 200, 3)\n",
      "185 (200, 200, 3)\n",
      "186 (200, 200, 3)\n",
      "187 (200, 200, 3)\n",
      "188 (200, 200, 3)\n",
      "189 (200, 200, 3)\n",
      "190 (200, 200, 3)\n",
      "191 (200, 200, 3)\n",
      "192 (200, 200, 3)\n",
      "193 (200, 200, 3)\n",
      "194 (200, 200, 3)\n",
      "195 (200, 200, 3)\n",
      "196 (200, 200, 3)\n",
      "197 (200, 200, 3)\n",
      "198 (200, 200, 3)\n",
      "199 (200, 200, 3)\n",
      "200 (200, 200, 3)\n",
      "201 (200, 200, 3)\n",
      "202 (200, 200, 3)\n",
      "203 (200, 200, 3)\n",
      "204 (200, 200, 3)\n",
      "205 (200, 200, 3)\n",
      "206 (200, 200, 3)\n",
      "207 (200, 200, 3)\n",
      "208 (200, 200, 3)\n",
      "209 (200, 200, 3)\n",
      "210 (200, 200, 3)\n",
      "211 (200, 200, 3)\n",
      "212 (200, 200, 3)\n",
      "213 (200, 200, 3)\n",
      "214 (200, 200, 3)\n",
      "215 (200, 200, 3)\n",
      "216 (200, 200, 3)\n",
      "217 (200, 200, 3)\n",
      "218 (200, 200, 3)\n",
      "219 (200, 200, 3)\n",
      "220 (200, 200, 3)\n",
      "221 (200, 200, 3)\n",
      "222 (200, 200, 3)\n",
      "223 (200, 200, 3)\n",
      "224 (200, 200, 3)\n",
      "225 (200, 200, 3)\n",
      "226 (200, 200, 3)\n",
      "227 (200, 200, 3)\n",
      "228 (200, 200, 3)\n",
      "229 (200, 200, 3)\n",
      "230 (200, 200, 3)\n",
      "231 (200, 200, 3)\n",
      "232 (200, 200, 3)\n",
      "233 (200, 200, 3)\n",
      "234 (200, 200, 3)\n",
      "235 (200, 200, 3)\n",
      "236 (200, 200, 3)\n",
      "237 (200, 200, 3)\n",
      "238 (200, 200, 3)\n",
      "239 (200, 200, 3)\n",
      "240 (200, 200, 3)\n",
      "241 (200, 200, 3)\n",
      "242 (200, 200, 3)\n",
      "243 (200, 200, 3)\n",
      "244 (200, 200, 3)\n",
      "245 (200, 200, 3)\n",
      "246 (200, 200, 3)\n",
      "247 (200, 200, 3)\n",
      "248 (200, 200, 3)\n",
      "249 (200, 200, 3)\n",
      "250 (200, 200, 3)\n",
      "251 (200, 200, 3)\n",
      "252 (200, 200, 3)\n",
      "253 (200, 200, 3)\n",
      "254 (200, 200, 3)\n",
      "255 (200, 200, 3)\n",
      "256 (200, 200, 3)\n",
      "257 (200, 200, 3)\n",
      "258 (200, 200, 3)\n",
      "259 (200, 200, 3)\n",
      "260 (200, 200, 3)\n",
      "261 (200, 200, 3)\n",
      "262 (200, 200, 3)\n",
      "263 (200, 200, 3)\n",
      "264 (200, 200, 3)\n",
      "265 (200, 200, 3)\n",
      "266 (200, 200, 3)\n",
      "267 (200, 200, 3)\n",
      "268 (200, 200, 3)\n",
      "269 (200, 200, 3)\n",
      "270 (200, 200, 3)\n",
      "271 (200, 200, 3)\n",
      "272 (200, 200, 3)\n",
      "273 (200, 200, 3)\n",
      "274 (200, 200, 3)\n",
      "275 (200, 200, 3)\n",
      "276 (200, 200, 3)\n",
      "277 (200, 200, 3)\n",
      "278 (200, 200, 3)\n",
      "279 (200, 200, 3)\n",
      "280 (200, 200, 3)\n",
      "281 (200, 200, 3)\n",
      "282 (200, 200, 3)\n",
      "283 (200, 200, 3)\n",
      "284 (200, 200, 1)\n",
      "285 (200, 200, 3)\n",
      "286 (200, 200, 3)\n",
      "287 (200, 200, 3)\n",
      "288 (200, 200, 3)\n",
      "289 (200, 200, 3)\n",
      "290 (200, 200, 3)\n",
      "291 (200, 200, 3)\n",
      "292 (200, 200, 3)\n",
      "293 (200, 200, 3)\n",
      "294 (200, 200, 3)\n",
      "295 (200, 200, 3)\n",
      "296 (200, 200, 3)\n",
      "297 (200, 200, 3)\n",
      "298 (200, 200, 3)\n",
      "299 (200, 200, 3)\n",
      "300 (200, 200, 3)\n",
      "301 (200, 200, 3)\n",
      "302 (200, 200, 3)\n",
      "303 (200, 200, 3)\n",
      "304 (200, 200, 3)\n",
      "305 (200, 200, 3)\n",
      "306 (200, 200, 3)\n",
      "307 (200, 200, 3)\n",
      "308 (200, 200, 3)\n",
      "309 (200, 200, 3)\n",
      "310 (200, 200, 3)\n",
      "311 (200, 200, 3)\n",
      "312 (200, 200, 3)\n"
     ]
    }
   ],
   "source": [
    "dataset_final = np.ndarray(shape=(1000,200, 200,3),dtype=np.float32)\n",
    "# Loading the image data\n",
    "for i in range(len(data)):\n",
    "    url=data[i]['images_url']\n",
    "    if url.startswith(\"//\"):\n",
    "        url =\"https://\"+url[2:]\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        img = Image.open(BytesIO(response.content))\n",
    "        # Making sure all images are of the same dimensions\n",
    "        if img.size!=(200,200):\n",
    "            img=img.resize((200, 200), PIL.Image.ANTIALIAS)\n",
    "        x = img_to_array(img)\n",
    "        # In case of grayScale images the len(img.shape) == 2\n",
    "        if len(x.shape) > 2 and x.shape[2] == 4:\n",
    "            #convert the image from RGBA2RGB\n",
    "            x = cv2.cvtColor(x, cv2.COLOR_BGRA2BGR)  \n",
    "    except:\n",
    "        x=img_to_array(blank_image())\n",
    "        pass\n",
    "    x = (x - 128.0) / 128.0\n",
    "    print(i,x.shape)\n",
    "    dataset_final[i]=x\n",
    "    #dataset[i] = np.rollaxis(x, axis=2, start=0)   # this is a Numpy array with shape (3, 200, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 3, 200, 200)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.9296875,  0.9296875,  0.9296875, ...,  0.9453125,  0.9453125,\n",
       "          0.9453125],\n",
       "        [ 0.9296875,  0.9296875,  0.9296875, ...,  0.9453125,  0.9453125,\n",
       "          0.9453125],\n",
       "        [ 0.9296875,  0.9296875,  0.9296875, ...,  0.9453125,  0.9453125,\n",
       "          0.9453125],\n",
       "        ..., \n",
       "        [ 0.9375   ,  0.9375   ,  0.9375   , ...,  0.9375   ,  0.9375   ,\n",
       "          0.9375   ],\n",
       "        [ 0.9375   ,  0.9375   ,  0.9375   , ...,  0.9375   ,  0.9375   ,\n",
       "          0.9375   ],\n",
       "        [ 0.9375   ,  0.9375   ,  0.9375   , ...,  0.9375   ,  0.9375   ,\n",
       "          0.9375   ]],\n",
       "\n",
       "       [[ 0.9296875,  0.9296875,  0.9296875, ...,  0.9453125,  0.9453125,\n",
       "          0.9453125],\n",
       "        [ 0.9296875,  0.9296875,  0.9296875, ...,  0.9453125,  0.9453125,\n",
       "          0.9453125],\n",
       "        [ 0.9296875,  0.9296875,  0.9296875, ...,  0.9453125,  0.9453125,\n",
       "          0.9453125],\n",
       "        ..., \n",
       "        [ 0.9375   ,  0.9375   ,  0.9375   , ...,  0.9375   ,  0.9375   ,\n",
       "          0.9375   ],\n",
       "        [ 0.9375   ,  0.9375   ,  0.9375   , ...,  0.9375   ,  0.9375   ,\n",
       "          0.9375   ],\n",
       "        [ 0.9375   ,  0.9375   ,  0.9375   , ...,  0.9375   ,  0.9375   ,\n",
       "          0.9375   ]],\n",
       "\n",
       "       [[ 0.9296875,  0.9296875,  0.9296875, ...,  0.9453125,  0.9453125,\n",
       "          0.9453125],\n",
       "        [ 0.9296875,  0.9296875,  0.9296875, ...,  0.9453125,  0.9453125,\n",
       "          0.9453125],\n",
       "        [ 0.9296875,  0.9296875,  0.9296875, ...,  0.9453125,  0.9453125,\n",
       "          0.9453125],\n",
       "        ..., \n",
       "        [ 0.9375   ,  0.9375   ,  0.9375   , ...,  0.9375   ,  0.9375   ,\n",
       "          0.9375   ],\n",
       "        [ 0.9375   ,  0.9375   ,  0.9375   , ...,  0.9375   ,  0.9375   ,\n",
       "          0.9375   ],\n",
       "        [ 0.9375   ,  0.9375   ,  0.9375   , ...,  0.9375   ,  0.9375   ,\n",
       "          0.9375   ]]], dtype=float32)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight_file = os.path.join(walk_up_folder(os.getcwd(), depth=1),'Data/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_base_model(weights_path):\n",
    "    \"\"\"\n",
    "    Returns the convolutional part of VGG net as a keras model \n",
    "    All layers have trainable set to False\n",
    "    \"\"\"\n",
    "    img_width, img_height = 200, 200\n",
    "\n",
    "    # build the VGG16 network\n",
    "    model = Sequential()\n",
    "    model.add(ZeroPadding2D((1, 1), input_shape=(3, img_width, img_height), name='image_input'))\n",
    "\n",
    "    model.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(64, 3, 3, activation='relu', name='conv1_2'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2), dim_ordering=\"th\"))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(128, 3, 3, activation='relu', name='conv2_2'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2), dim_ordering=\"th\"))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_2'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu', name='conv3_3'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2), dim_ordering=\"th\"))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_2'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv4_3'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2), dim_ordering=\"th\"))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_1'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_2'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu', name='conv5_3'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2), dim_ordering=\"th\"))\n",
    "\n",
    "    # set trainable to false in all layers \n",
    "    for layer in model.layers:\n",
    "        if hasattr(layer, 'trainable'):\n",
    "            layer.trainable = False\n",
    "    if weights_path:\n",
    "            model.load_weights(weights_path)\n",
    "\n",
    "        \n",
    "    return model\n",
    "\n",
    "def load_weights_in_base_model(model):\n",
    "    \"\"\"\n",
    "    The function takes the VGG convolutian part and loads\n",
    "    the weights from the pre-trained model and then returns the model\n",
    "    \"\"\"\n",
    "    weight_file = os.path.join(walk_up_folder(os.getcwd(), depth=1),'Data/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5')\n",
    "    f = h5py.File(weight_file)\n",
    "    for k in range(f.attrs['nb_layers']):\n",
    "        if k >= len(model.layers):\n",
    "            # we don't look at the last (fully-connected) layers in the savefile\n",
    "            break\n",
    "        g = f['layer_{}'.format(k)]\n",
    "        weights = [g['param_{}'.format(p)] for p in range(g.attrs['nb_params'])]\n",
    "        model.layers[k].set_weights(weights)\n",
    "    f.close()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Dimension 0 in both shapes must be equal, but are 3 and 64. Shapes are [3,3,200,64] and [64,3,3,3]. for 'Assign_2' (op: 'Assign') with input shapes: [3,3,200,64], [64,3,3,3].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/envs/tfdeeplearning/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1588\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1589\u001b[0;31m     \u001b[0mc_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1590\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Dimension 0 in both shapes must be equal, but are 3 and 64. Shapes are [3,3,200,64] and [64,3,3,3]. for 'Assign_2' (op: 'Assign') with input shapes: [3,3,200,64], [64,3,3,3].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-169-99b47afbd0be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_base_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-168-fe9525ebd555>\u001b[0m in \u001b[0;36mget_base_model\u001b[0;34m(weights_path)\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mweights_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tfdeeplearning/lib/python3.5/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, reshape)\u001b[0m\n\u001b[1;32m   1159\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m                 saving.load_weights_from_hdf5_group(\n\u001b[0;32m-> 1161\u001b[0;31m                     f, self.layers, reshape=reshape)\n\u001b[0m\u001b[1;32m   1162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_updated_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tfdeeplearning/lib/python3.5/site-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[0;34m(f, layers, reshape)\u001b[0m\n\u001b[1;32m    926\u001b[0m                              ' elements.')\n\u001b[1;32m    927\u001b[0m         \u001b[0mweight_value_tuples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbolic_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 928\u001b[0;31m     \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_value_tuples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tfdeeplearning/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mbatch_set_value\u001b[0;34m(tuples)\u001b[0m\n\u001b[1;32m   2433\u001b[0m                 assign_placeholder = tf.placeholder(tf_dtype,\n\u001b[1;32m   2434\u001b[0m                                                     shape=value.shape)\n\u001b[0;32m-> 2435\u001b[0;31m                 \u001b[0massign_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_placeholder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2436\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assign_placeholder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0massign_placeholder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2437\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assign_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0massign_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tfdeeplearning/lib/python3.5/site-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36massign\u001b[0;34m(self, value, use_locking)\u001b[0m\n\u001b[1;32m    643\u001b[0m       \u001b[0mthe\u001b[0m \u001b[0massignment\u001b[0m \u001b[0mhas\u001b[0m \u001b[0mcompleted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m     \"\"\"\n\u001b[0;32m--> 645\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mstate_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_locking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_locking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0massign_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_locking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tfdeeplearning/lib/python3.5/site-packages/tensorflow/python/ops/state_ops.py\u001b[0m in \u001b[0;36massign\u001b[0;34m(ref, value, validate_shape, use_locking, name)\u001b[0m\n\u001b[1;32m    217\u001b[0m     return gen_state_ops.assign(\n\u001b[1;32m    218\u001b[0m         \u001b[0mref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_locking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_locking\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m         validate_shape=validate_shape)\n\u001b[0m\u001b[1;32m    220\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tfdeeplearning/lib/python3.5/site-packages/tensorflow/python/ops/gen_state_ops.py\u001b[0m in \u001b[0;36massign\u001b[0;34m(ref, value, validate_shape, use_locking, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[1;32m     59\u001b[0m         \u001b[0;34m\"Assign\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         use_locking=use_locking, name=name)\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tfdeeplearning/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    785\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    786\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    788\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tfdeeplearning/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   3412\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3413\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3414\u001b[0;31m           op_def=op_def)\n\u001b[0m\u001b[1;32m   3415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3416\u001b[0m       \u001b[0;31m# Note: shapes are lazily computed with the C API enabled.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tfdeeplearning/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   1754\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[1;32m   1755\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[0;32m-> 1756\u001b[0;31m                                 control_input_ops)\n\u001b[0m\u001b[1;32m   1757\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1758\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tfdeeplearning/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1590\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1591\u001b[0m     \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1592\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1594\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Dimension 0 in both shapes must be equal, but are 3 and 64. Shapes are [3,3,200,64] and [64,3,3,3]. for 'Assign_2' (op: 'Assign') with input shapes: [3,3,200,64], [64,3,3,3]."
     ]
    }
   ],
   "source": [
    "model = get_base_model(weight_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Flatten, Dense, Dropout\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.optimizers import SGD\n",
    "import cv2, numpy as np\n",
    "\n",
    "def VGG_16(weights_path=None):\n",
    "    model = Sequential()\n",
    "    model.add(ZeroPadding2D((1,1),input_shape=(3,200,200)))\n",
    "    model.add(Convolution2D(64, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(64, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2), dim_ordering=\"th\"))\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(128, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(128, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2), dim_ordering=\"th\"))\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2), dim_ordering=\"th\"))\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2), dim_ordering=\"th\"))\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2), dim_ordering=\"th\"))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    #model.add(Dense(4096, activation='relu'))\n",
    "    #model.add(Dropout(0.5))\n",
    "    #model.add(Dense(4096, activation='relu'))\n",
    "    #model.add(Dropout(0.5))\n",
    "    #model.add(Dense(1000, activation='softmax'))\n",
    "\n",
    "    if weights_path:\n",
    "        model.load_weights(weights_path)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = VGG_16(weight_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.applications import VGG19\n",
    "from keras.applications.vgg19 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "80142336/80134624 [==============================] - 16s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Create the base model of VGG19\n",
    "vgg19 = VGG19(weights='imagenet', include_top=False, input_shape = (200, 200, 3), classes = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = preprocess_input(dataset_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_features = vgg19.predict(np.array(X_train), batch_size=256, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
